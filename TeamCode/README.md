# TeamCode Module by FTC Team #15173

## Object recognition
We have used 2 methods of detecting objects. 
* First is based on TensorFlow
* The other one is based on [EasyOpenCV](https://github.com/OpenFTC/EasyOpenCV)


### TensorFlow
This method is based on machine learning. You train a model and use it to recognize objects in the camera stream.
We use [Teachable Machine](https://teachablemachine.withgoogle.com/train) to create TensorFlow compatible models. 
Watch [this video](https://www.youtube.com/watch?v=aeMWWvteF2U) to see how it was done in the Ultimate goal season.

Once you have built your model in the Teachable Machine, save it in **tflite floating** format.
The FTC implementation of tensorFlow processing last year did not work with `tflite` models generated by the Teachable Machine. For that reason we created our own module using several Google examples.
It is packaged in the [`tfrec` folder under TeamCode](https://github.com/MHSRoboticEagles/FtcRobotController/tree/1ad8e47f8932f2b5aa500d87c877d4dcd73968a8/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/tfrec)


If you want to give it a try, do the following:

* Copy `tfrec` folder from our TeamCode repo and paste it into your TeamCode folder

* Locate build.dependencies.gradle file and add these lines to the dependencies block:
```
dependencies{
….

implementation('org.tensorflow:tensorflow-lite:2.0.0')
implementation('org.tensorflow:tensorflow-lite-gpu:2.0.0')
implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly') { changing = true }
}
```

Last year we had to remove references to the older versions of the tensorflow-lite libraries that FTC uses. 
This year, there seems to be no conflicts, as FTC packaged everything in a different library. 

* Create `assets` folder under your TeamCode folder, unless you already have it
* Copy the files generated by the Teachable Machine into your `assets` folder. Each model is represented by 2 files:
    *.tflite file with the model
    *.txt file with labels.
It does not matter what you call them since you will be referencing the files by name in your code.

* For an example of how to consume the detection, take a look at [TeamCode/skills/GenericDetector](https://github.com/MHSRoboticEagles/FtcRobotController/blob/1ad8e47f8932f2b5aa500d87c877d4dcd73968a8/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/skills/GenericDetector.java) class.
It is a wrapper of the tflite engine that we have in `tfrec` folder. This class is specific to the last season. Note that it is a runnable, to run on a separate thread. This is where we reference the file names of our model. Make sure to change them based on your model file names.
You can use [TeamCode/OpMOdes/GenericRecognitionTest](https://github.com/MHSRoboticEagles/FtcRobotController/blob/1ad8e47f8932f2b5aa500d87c877d4dcd73968a8/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/OpModes/GenericRecognitionTest.java) as an example opmode that consumes the GenericDetector.
Both of these generic classes are meant to be a starting point from which you can build up the recognition logic for your specific needs.

Note that we initialize the detector in the “init” phase of the Op mode. 
It usually takes 3-5 seconds for the tflite engine to “warm up”. The idea is that by the time you press Start, the detection results are available.

### EasyOpenCV

[EasyOpenCV](https://github.com/OpenFTC/EasyOpenCV) is based on color analysis in the camera stream.
You define one or more viewport areas, typically described as rectangles, and let EasyOpenCV analyze the input and manipulate color channels. The analysis boils down to comparing the observed values with expected values that are specific to color(s) of the objects that you need to detect.
The detection logic is implemented in a pipeline class. 

In the 2021-22 season we are using [CVFrenzyPipeline](https://github.com/MHSRoboticEagles/FtcRobotController/blob/1ad8e47f8932f2b5aa500d87c877d4dcd73968a8/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CVRec/CVFrenzyPipeline.java)

[CVRingSearchPipeline](https://github.com/MHSRoboticEagles/FtcRobotController/blob/1ad8e47f8932f2b5aa500d87c877d4dcd73968a8/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/CVRec/CVRingSearchPipeline.java) provides a more complex use case, where we split the viewport in multiple rectangles to detect the location of the object relative to the camera on the robot.

## Odometry

We use two different methods of [odometry](https://en.wikipedia.org/wiki/Odometry)
1. Based on dead-wheel encoders
2. Based on a VSLAM Camera module

### Dead-Wheel encoders based odometry

<p>
    <img src="https://d2t1xqejof9utc.cloudfront.net/screenshots/pics/8ea4c81c560b3f2f6fdb75d43c890e09/small.JPG"/>
</p>

This method is very effective and relies on three free-spinning encoder wheels dragging along on the ground. 
Two of the wheels are aligned front-to-back and the third is aligned left-to-right with respect to the robot frame.
Each wheel precisely tells us how much it has moved. 

Here is a good writeup on [Dead Wheels](https://gm0.org/en/latest/docs/robot-design/dead-wheels.html) on gm-zero website.

Using some high-school level math, we can accurately deduce how much the robot has moved from its start position and hence the exact location of the robot on the playing field.
(See [Mecanum Wheel Odometry](https://chsftcrobotics.weebly.com/uploads/1/2/3/6/123696510/odometry.pdf) by FTC 9866 Virus for an excellent primer on how the calculations work.)


### VLSM Camera based odometry

<p>
    <img src="https://www.intelrealsense.com/wp-content/uploads/2019/02/intel_realsense_tracking_camera_photo_angle_1_675x450.png" width="300px"/>
</p>

Intel's [RealSense Tracking Camera T265](https://www.intelrealsense.com/tracking-camera-t265/) is a simple USB camera that uses on-board vision to precisely determine its own position.
The technology is called VSLAM (Visual SLAM). See [Simultaneous localization and mapping](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)

GitHub user `pietroglyph` has wrapped the odometry library for use with FTC. 
The library and instructions can be found on GitHub at [pietroglyph/ftc265](https://github.com/pietroglyph/ftc265) 

In our codebase, the [VSLAMOdometry](https://github.com/MHSRoboticEagles/FtcRobotController/blob/master/TeamCode/src/main/java/org/firstinspires/ftc/teamcode/odometry/VSlamOdometry.java) class encapsulates the implementation. Upon startup, this class initializes the camera and starts a background thread that continually polls the Tracking Camera for updated location. Public methods are exposed to make this information available to other classes.

